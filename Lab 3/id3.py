# -*- coding: utf-8 -*-
"""ID3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytAZ1slIQjFiUC4CvyLuzBQNX2VIjEOp
"""

import pandas as pd
import numpy as np
import math
data=pd.read_csv('/content/data set-1.csv');

attributes=[feat for feat in data]
attributes.remove('answer')
# print(features)
class Node:
  def __init__(self):
    self.children=[];
    self.isLeaf=False;
    self.value="";
    self.pred="";

def main():
  res=ID3(data,attributes)
  printTree(res)

def printTree(root: Node, depth=0):
    for i in range(depth):
        print("\t", end="")
    
    print(root.value, end="")
    
    if root.isLeaf:
        print(" ->", root.pred)
    
    print()
    
    for child in root.children:
        printTree(child, depth + 1)

# This function creates the decision tree recursively
def ID3(data_set,attributes):
  root=Node()
  max_gain=0.0;
  max_feat="";
  # Comparitively find out which attribute gives us the maximum information
  for attribute in attributes:
    gain=info_gain(data_set,attribute)
    if gain>max_gain:
      max_gain=gain
      max_feat=attribute
  # once we find the max gain, that will be the attribute which we use.
  root.value=max_feat

  # All types of a particular attribute. Ex: In outlook, we have sunny,rain,overcast 
  types=np.unique(data_set[max_feat])

  for t in types:
    # Get all instances which match a particular type
    subdata=data_set[data_set[max_feat]==t]

    # In case we find instances where we have only one type of data result (yes/no). Entropy will be zero (Obviously!!)
    if entropy(subdata)==0.0:
      newNode=Node()
      newNode.isLeaf=True
      newNode.value=t
      newNode.pred=np.unique(subdata["answer"])
      root.children.append(newNode)
    else:
      # If even one instance has different type of data result, we still cannot come to conclusion, 
      # hence go to the next attribute and create the node and apply the same algorithm on the next attribute.
      dummyNode=Node()
      dummyNode.value=t
      new_attr=attributes.copy()
      # We can remove the current attribute, only when we have come to a conclusion 
      # that we cannot decide with this attribute, we have gone to the next attribute. Hence we don't want to come back.
      # + we may get stuck in cycle.
      new_attr.remove(max_feat)

      # Apply the algorithm on the next attribute with same current attributes which have been deleted.
      child=ID3(subdata,new_attr)
      dummyNode.children.append(child)
      root.children.append(dummyNode)
  return root


def info_gain(data_set,feature):
  types=np.unique(data_set[feature])
  # We are trying to get the entropy for the entire data_set we have taken into consideration. 
  gain=entropy(data_set)

  for u in types:
    subdata=data_set[data_set[feature]==u]
    sub_e=entropy(subdata)
    gain-=(float(len(subdata))/float(len(data_set))*sub_e)

  return gain

def entropy(data):
  pos=0
  neg=0
  # For the formula of entropy we need to see for how many of the +ve samples (yes) we have and how many -ve samples(no).
  for _, row in data.iterrows():
        if row['answer'] == "yes":
            pos += 1
        else:
            neg += 1
  if pos==0.0 or neg==0.0:
    return 0.0
  p=pos/(pos+neg)
  n=neg/(pos+neg)
  return -(p*math.log(p,2)+n*math.log(n,2))
main()